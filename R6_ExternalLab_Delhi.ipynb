{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_Delhi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYk8NG3yOIT9",
        "colab_type": "text"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFO6PuxzOIT_",
        "colab_type": "text"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efNjNImfOIUC",
        "colab_type": "code",
        "outputId": "fe7c9761-b8b7-440b-c52c-52c88911ac29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9C4aAIGOIUH",
        "colab_type": "code",
        "outputId": "9d60a4a2-c14e-4dc6-95df-cc371974e6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcoZBStrOIUQ",
        "colab_type": "text"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1WsFSeOIUS",
        "colab_type": "code",
        "outputId": "3048edd6-4856-44cb-ad36-e27a2bd99c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnbx7TyQOIUY",
        "colab_type": "code",
        "outputId": "c3af0720-8f4a-4be3-fac5-204bc56d119d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "40960/29515 [=========================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "26435584/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n",
            "4431872/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbiHj5YPOIUc",
        "colab_type": "code",
        "outputId": "a157f135-9f1f-49aa-930d-0609e3d62277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0eMb9I9-do",
        "colab_type": "code",
        "outputId": "7d5d8eea-16c5-4616-a02a-eab1c5aba2a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtotIB7N-PvW",
        "colab_type": "code",
        "outputId": "9a2cc428-27ae-44e3-94d0-13446547229c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainY"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDAYzkwyOIUj",
        "colab_type": "text"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBlfYlANOIUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf_xOUfk_7m1",
        "colab_type": "code",
        "outputId": "7efbb83a-b247-4c9a-a984-735cf27a5b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RHV3b9mzOIUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9YIxrQwAMWv",
        "colab_type": "code",
        "outputId": "434f1c2f-d7cf-4f16-a174-c7aa340968ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(testY.shape)\n",
        "print('First 5 examples now are: ', testY[0:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n",
            "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwhQ8e7VOIUw",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1anNbB6dCUid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWzpuepBHN2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOzHgkdgEyoW",
        "colab_type": "code",
        "outputId": "eed4a8d6-33ee-44ce-efbd-0c4f300ae2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "columns = 10\n",
        "for i in range(0,10):\n",
        "    plt.subplot(10 / columns + 1, columns, i + 1)\n",
        "    plt.imshow(trainX[i],cmap='gray',)\n",
        "    labels.append(np.argmax(trainY[i]))\n",
        "print('Labels for each image')\n",
        "print(labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels for each image\n",
            "[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABSCAYAAABwglFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTplyOBwOh8PhyIP/DwwJUGa9MVLYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4TbJGeSOIU4",
        "colab_type": "text"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac06XZZTOIU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "228679db-1119-4e5f-f7ff-25e3acaf463d"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1110 09:13:35.447760 140128811198336 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hQpLv3aOIU_",
        "colab_type": "text"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilz1D-USJCNa",
        "colab_type": "code",
        "outputId": "697c3a8e-64dd-4adf-a46e-69a9ff6ed737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=50,\n",
        "          batch_size=30)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 2080.5945 - acc: 0.7409 - val_loss: 1289.8372 - val_acc: 0.7991\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1646.9322 - acc: 0.7807 - val_loss: 1086.4038 - val_acc: 0.8050\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1621.4382 - acc: 0.7837 - val_loss: 1954.6833 - val_acc: 0.8006\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 1569.0645 - acc: 0.7899 - val_loss: 1507.5453 - val_acc: 0.7741\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1512.1228 - acc: 0.7941 - val_loss: 1320.6494 - val_acc: 0.7761\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1533.5692 - acc: 0.7936 - val_loss: 1424.9272 - val_acc: 0.8060\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1524.0036 - acc: 0.7935 - val_loss: 5598.5547 - val_acc: 0.6111\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 1471.0590 - acc: 0.7997 - val_loss: 1468.9310 - val_acc: 0.7799\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1480.7862 - acc: 0.7994 - val_loss: 1413.8090 - val_acc: 0.8057\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1548.1042 - acc: 0.7971 - val_loss: 1449.1784 - val_acc: 0.7815\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1446.5238 - acc: 0.8020 - val_loss: 1068.0463 - val_acc: 0.8185\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1453.0998 - acc: 0.8027 - val_loss: 1558.4929 - val_acc: 0.7988\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1469.4297 - acc: 0.7999 - val_loss: 1266.3944 - val_acc: 0.7987\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1474.4548 - acc: 0.8036 - val_loss: 1562.5027 - val_acc: 0.7847\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1431.1635 - acc: 0.8039 - val_loss: 1361.6821 - val_acc: 0.7934\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 1434.6843 - acc: 0.8035 - val_loss: 1085.9692 - val_acc: 0.8230\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1398.3206 - acc: 0.8043 - val_loss: 1270.2909 - val_acc: 0.8234\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1444.6700 - acc: 0.8037 - val_loss: 1151.4994 - val_acc: 0.8056\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1451.4613 - acc: 0.8026 - val_loss: 1940.7028 - val_acc: 0.7717\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1378.3332 - acc: 0.8076 - val_loss: 1813.1069 - val_acc: 0.7974\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 2s 40us/sample - loss: 1416.6632 - acc: 0.8072 - val_loss: 1060.9803 - val_acc: 0.8242\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1437.3701 - acc: 0.8075 - val_loss: 2218.2049 - val_acc: 0.7989\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1397.5424 - acc: 0.8064 - val_loss: 1258.9842 - val_acc: 0.7904\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 2s 40us/sample - loss: 1487.9099 - acc: 0.8054 - val_loss: 1171.5770 - val_acc: 0.8190\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1437.1536 - acc: 0.8070 - val_loss: 1378.6469 - val_acc: 0.8111\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1432.2203 - acc: 0.8065 - val_loss: 1589.2873 - val_acc: 0.7778\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1374.6744 - acc: 0.8075 - val_loss: 1232.6240 - val_acc: 0.8015\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 2s 40us/sample - loss: 1447.7961 - acc: 0.8062 - val_loss: 1150.6910 - val_acc: 0.8222\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1429.9822 - acc: 0.8064 - val_loss: 1589.0636 - val_acc: 0.7908\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1451.3756 - acc: 0.8058 - val_loss: 3040.8852 - val_acc: 0.6969\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1431.7970 - acc: 0.8069 - val_loss: 1735.9922 - val_acc: 0.7714\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1409.0761 - acc: 0.8057 - val_loss: 1759.0467 - val_acc: 0.7618\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1368.4770 - acc: 0.8105 - val_loss: 1686.1547 - val_acc: 0.7337\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1437.5948 - acc: 0.8062 - val_loss: 1504.4788 - val_acc: 0.7915\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1389.9231 - acc: 0.8092 - val_loss: 1699.8092 - val_acc: 0.7828\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1377.3684 - acc: 0.8113 - val_loss: 3064.4660 - val_acc: 0.7299\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1412.3703 - acc: 0.8090 - val_loss: 1292.8725 - val_acc: 0.8256\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1426.9187 - acc: 0.8079 - val_loss: 1272.0826 - val_acc: 0.8067\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1402.2033 - acc: 0.8092 - val_loss: 1146.3434 - val_acc: 0.8239\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1410.3147 - acc: 0.8096 - val_loss: 1890.7386 - val_acc: 0.7562\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1416.3106 - acc: 0.8099 - val_loss: 1260.6003 - val_acc: 0.8131\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 2s 41us/sample - loss: 1431.0560 - acc: 0.8085 - val_loss: 1565.2343 - val_acc: 0.7743\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1388.2631 - acc: 0.8111 - val_loss: 1295.7132 - val_acc: 0.8145\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1424.7705 - acc: 0.8080 - val_loss: 1647.6065 - val_acc: 0.7709\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1391.1995 - acc: 0.8115 - val_loss: 1700.0397 - val_acc: 0.7925\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 1390.2772 - acc: 0.8109 - val_loss: 2065.1069 - val_acc: 0.7638\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1409.9364 - acc: 0.8096 - val_loss: 1287.9408 - val_acc: 0.8115\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1382.7752 - acc: 0.8091 - val_loss: 1625.3026 - val_acc: 0.7803\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1374.7176 - acc: 0.8122 - val_loss: 2194.0280 - val_acc: 0.7613\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 1371.5133 - acc: 0.8114 - val_loss: 1932.9637 - val_acc: 0.7653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f720caae310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdzDtGwDOIVF",
        "colab_type": "text"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndfpdidOIVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwk3T5LJOIVN",
        "colab_type": "text"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNLR8tcBOIVP",
        "colab_type": "code",
        "outputId": "f38375cf-b65c-44b9-a2d7-c473e1a19878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=50,\n",
        "          batch_size=30)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.5967 - acc: 0.7936 - val_loss: 0.5150 - val_acc: 0.8262\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4922 - acc: 0.8298 - val_loss: 0.5083 - val_acc: 0.8282\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4733 - acc: 0.8365 - val_loss: 0.4878 - val_acc: 0.8338\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4593 - acc: 0.8416 - val_loss: 0.4706 - val_acc: 0.8385\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4526 - acc: 0.8428 - val_loss: 0.4690 - val_acc: 0.8389\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4482 - acc: 0.8436 - val_loss: 0.4797 - val_acc: 0.8400\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4403 - acc: 0.8479 - val_loss: 0.4770 - val_acc: 0.8363\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4363 - acc: 0.8477 - val_loss: 0.4662 - val_acc: 0.8413\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4336 - acc: 0.8492 - val_loss: 0.4757 - val_acc: 0.8355\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4328 - acc: 0.8499 - val_loss: 0.4647 - val_acc: 0.8416\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4273 - acc: 0.8506 - val_loss: 0.4771 - val_acc: 0.8417\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4296 - acc: 0.8506 - val_loss: 0.4633 - val_acc: 0.8425\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4268 - acc: 0.8503 - val_loss: 0.4730 - val_acc: 0.8412\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4269 - acc: 0.8504 - val_loss: 0.4666 - val_acc: 0.8438\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4266 - acc: 0.8515 - val_loss: 0.4686 - val_acc: 0.8394\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4223 - acc: 0.8519 - val_loss: 0.4612 - val_acc: 0.8437\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4221 - acc: 0.8521 - val_loss: 0.4850 - val_acc: 0.8400\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4232 - acc: 0.8511 - val_loss: 0.4624 - val_acc: 0.8405\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4193 - acc: 0.8515 - val_loss: 0.4696 - val_acc: 0.8395\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4207 - acc: 0.8517 - val_loss: 0.4753 - val_acc: 0.8400\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4183 - acc: 0.8526 - val_loss: 0.4663 - val_acc: 0.8434\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4157 - acc: 0.8546 - val_loss: 0.4726 - val_acc: 0.8388\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4184 - acc: 0.8537 - val_loss: 0.4604 - val_acc: 0.8441\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4166 - acc: 0.8530 - val_loss: 0.4759 - val_acc: 0.8425\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4174 - acc: 0.8533 - val_loss: 0.4698 - val_acc: 0.8424\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4164 - acc: 0.8544 - val_loss: 0.4664 - val_acc: 0.8421\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4134 - acc: 0.8553 - val_loss: 0.4727 - val_acc: 0.8417\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4146 - acc: 0.8540 - val_loss: 0.4753 - val_acc: 0.8395\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4134 - acc: 0.8548 - val_loss: 0.5133 - val_acc: 0.8322\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4135 - acc: 0.8544 - val_loss: 0.4626 - val_acc: 0.8426\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4153 - acc: 0.8545 - val_loss: 0.4892 - val_acc: 0.8416\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4125 - acc: 0.8555 - val_loss: 0.4665 - val_acc: 0.8413\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4116 - acc: 0.8561 - val_loss: 0.4705 - val_acc: 0.8434\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4129 - acc: 0.8549 - val_loss: 0.4889 - val_acc: 0.8368\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4112 - acc: 0.8550 - val_loss: 0.4736 - val_acc: 0.8429\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4095 - acc: 0.8563 - val_loss: 0.4700 - val_acc: 0.8411\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4106 - acc: 0.8551 - val_loss: 0.4984 - val_acc: 0.8396\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4132 - acc: 0.8536 - val_loss: 0.4736 - val_acc: 0.8420\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4110 - acc: 0.8545 - val_loss: 0.4891 - val_acc: 0.8384\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4112 - acc: 0.8549 - val_loss: 0.4870 - val_acc: 0.8424\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4099 - acc: 0.8562 - val_loss: 0.4849 - val_acc: 0.8404\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4076 - acc: 0.8558 - val_loss: 0.4838 - val_acc: 0.8411\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4083 - acc: 0.8554 - val_loss: 0.4873 - val_acc: 0.8422\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4075 - acc: 0.8577 - val_loss: 0.4770 - val_acc: 0.8430\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4096 - acc: 0.8561 - val_loss: 0.4653 - val_acc: 0.8416\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4087 - acc: 0.8567 - val_loss: 0.4713 - val_acc: 0.8393\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4082 - acc: 0.8561 - val_loss: 0.4880 - val_acc: 0.8397\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4095 - acc: 0.8554 - val_loss: 0.4963 - val_acc: 0.8372\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4051 - acc: 0.8555 - val_loss: 0.4849 - val_acc: 0.8326\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4075 - acc: 0.8564 - val_loss: 0.4683 - val_acc: 0.8414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f720f9f3890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py-KwkmjOIVU",
        "colab_type": "text"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLXUE9jWOIVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJUqA5T4OIVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb15c949-b26f-4607-8e89-48197283abc8"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=50,\n",
        "          batch_size=30)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.9245 - acc: 0.6803 - val_loss: 0.6933 - val_acc: 0.7651\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.6396 - acc: 0.7798 - val_loss: 0.6124 - val_acc: 0.7927\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.5861 - acc: 0.7967 - val_loss: 0.5882 - val_acc: 0.8047\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.5554 - acc: 0.8075 - val_loss: 0.5697 - val_acc: 0.8112\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.5377 - acc: 0.8135 - val_loss: 0.5365 - val_acc: 0.8176\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.5264 - acc: 0.8180 - val_loss: 0.5356 - val_acc: 0.8210\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.5136 - acc: 0.8230 - val_loss: 0.5182 - val_acc: 0.8244\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.5089 - acc: 0.8233 - val_loss: 0.5138 - val_acc: 0.8261\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4995 - acc: 0.8278 - val_loss: 0.5105 - val_acc: 0.8273\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4950 - acc: 0.8295 - val_loss: 0.5106 - val_acc: 0.8294\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4897 - acc: 0.8305 - val_loss: 0.4997 - val_acc: 0.8300\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4847 - acc: 0.8339 - val_loss: 0.5054 - val_acc: 0.8318\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4823 - acc: 0.8336 - val_loss: 0.5035 - val_acc: 0.8316\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4801 - acc: 0.8345 - val_loss: 0.4987 - val_acc: 0.8340\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4731 - acc: 0.8361 - val_loss: 0.4928 - val_acc: 0.8341\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4720 - acc: 0.8368 - val_loss: 0.4905 - val_acc: 0.8346\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4690 - acc: 0.8374 - val_loss: 0.4899 - val_acc: 0.8343\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4659 - acc: 0.8399 - val_loss: 0.4856 - val_acc: 0.8341\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4639 - acc: 0.8401 - val_loss: 0.4891 - val_acc: 0.8362\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4623 - acc: 0.8413 - val_loss: 0.4985 - val_acc: 0.8351\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4599 - acc: 0.8421 - val_loss: 0.4810 - val_acc: 0.8361\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4607 - acc: 0.8406 - val_loss: 0.4798 - val_acc: 0.8359\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4578 - acc: 0.8427 - val_loss: 0.4748 - val_acc: 0.8368\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4575 - acc: 0.8429 - val_loss: 0.4818 - val_acc: 0.8384\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4546 - acc: 0.8425 - val_loss: 0.4771 - val_acc: 0.8380\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4534 - acc: 0.8443 - val_loss: 0.4718 - val_acc: 0.8380\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4506 - acc: 0.8449 - val_loss: 0.4835 - val_acc: 0.8400\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4515 - acc: 0.8449 - val_loss: 0.4759 - val_acc: 0.8390\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4491 - acc: 0.8448 - val_loss: 0.4749 - val_acc: 0.8383\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4492 - acc: 0.8457 - val_loss: 0.4891 - val_acc: 0.8405\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4461 - acc: 0.8454 - val_loss: 0.4787 - val_acc: 0.8394\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4458 - acc: 0.8465 - val_loss: 0.4744 - val_acc: 0.8396\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4450 - acc: 0.8463 - val_loss: 0.4818 - val_acc: 0.8388\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4426 - acc: 0.8480 - val_loss: 0.4806 - val_acc: 0.8401\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4420 - acc: 0.8471 - val_loss: 0.4819 - val_acc: 0.8397\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4413 - acc: 0.8470 - val_loss: 0.4724 - val_acc: 0.8405\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4411 - acc: 0.8472 - val_loss: 0.4619 - val_acc: 0.8405\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4407 - acc: 0.8465 - val_loss: 0.4802 - val_acc: 0.8408\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4401 - acc: 0.8479 - val_loss: 0.4659 - val_acc: 0.8419\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4366 - acc: 0.8491 - val_loss: 0.4770 - val_acc: 0.8419\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4371 - acc: 0.8497 - val_loss: 0.4704 - val_acc: 0.8424\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4368 - acc: 0.8496 - val_loss: 0.4717 - val_acc: 0.8424\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4363 - acc: 0.8487 - val_loss: 0.4683 - val_acc: 0.8428\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4368 - acc: 0.8493 - val_loss: 0.4599 - val_acc: 0.8417\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4356 - acc: 0.8503 - val_loss: 0.4666 - val_acc: 0.8420\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4362 - acc: 0.8499 - val_loss: 0.4770 - val_acc: 0.8423\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4339 - acc: 0.8501 - val_loss: 0.4645 - val_acc: 0.8419\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4353 - acc: 0.8489 - val_loss: 0.4626 - val_acc: 0.8433\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4329 - acc: 0.8514 - val_loss: 0.4660 - val_acc: 0.8422\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4356 - acc: 0.8489 - val_loss: 0.4824 - val_acc: 0.8418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f71feb06650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CSqKvpOIVk",
        "colab_type": "text"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGAad54JOIVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "model.add(tf.keras.layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ7oIymROIVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-O-fFxnOIVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiP7IL52OIVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2YsZV0OIV0",
        "colab_type": "text"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ojW6-oOIV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "264bd3e3-0448-4a93-ebf2-7438da0ea314"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_3 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 92,856\n",
            "Trainable params: 91,288\n",
            "Non-trainable params: 1,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFGmbZLOIV5",
        "colab_type": "text"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIkbMEN5OIV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65955c78-600f-4a6e-bcf7-14718edb7abd"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=50,\n",
        "          batch_size=30)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 1.8835 - acc: 0.3426 - val_loss: 1.4802 - val_acc: 0.5848\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 1.1921 - acc: 0.6306 - val_loss: 0.9630 - val_acc: 0.7023\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.8439 - acc: 0.7307 - val_loss: 0.7175 - val_acc: 0.7607\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6824 - acc: 0.7650 - val_loss: 0.6173 - val_acc: 0.7830\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6074 - acc: 0.7837 - val_loss: 0.5665 - val_acc: 0.7944\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5612 - acc: 0.7961 - val_loss: 0.5332 - val_acc: 0.8028\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5267 - acc: 0.8063 - val_loss: 0.5082 - val_acc: 0.8109\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4991 - acc: 0.8197 - val_loss: 0.4840 - val_acc: 0.8258\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4680 - acc: 0.8378 - val_loss: 0.4558 - val_acc: 0.8431\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.4426 - acc: 0.8485 - val_loss: 0.4406 - val_acc: 0.8465\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4212 - acc: 0.8542 - val_loss: 0.4275 - val_acc: 0.8508\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4037 - acc: 0.8602 - val_loss: 0.4107 - val_acc: 0.8559\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3906 - acc: 0.8637 - val_loss: 0.4063 - val_acc: 0.8586\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3769 - acc: 0.8687 - val_loss: 0.4047 - val_acc: 0.8582\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3646 - acc: 0.8733 - val_loss: 0.3946 - val_acc: 0.8605\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3520 - acc: 0.8767 - val_loss: 0.3820 - val_acc: 0.8659\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.3457 - acc: 0.8800 - val_loss: 0.3933 - val_acc: 0.8608\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3352 - acc: 0.8834 - val_loss: 0.3686 - val_acc: 0.8715\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.3300 - acc: 0.8853 - val_loss: 0.3661 - val_acc: 0.8716\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3212 - acc: 0.8871 - val_loss: 0.3795 - val_acc: 0.8646\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3130 - acc: 0.8895 - val_loss: 0.3601 - val_acc: 0.8701\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.3098 - acc: 0.8918 - val_loss: 0.3628 - val_acc: 0.8705\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.3018 - acc: 0.8934 - val_loss: 0.3588 - val_acc: 0.8741\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2967 - acc: 0.8961 - val_loss: 0.3549 - val_acc: 0.8740\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2914 - acc: 0.8980 - val_loss: 0.3488 - val_acc: 0.8774\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2896 - acc: 0.8981 - val_loss: 0.3585 - val_acc: 0.8740\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2809 - acc: 0.9010 - val_loss: 0.3489 - val_acc: 0.8778\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.2777 - acc: 0.9014 - val_loss: 0.3521 - val_acc: 0.8773\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.2718 - acc: 0.9032 - val_loss: 0.3536 - val_acc: 0.8748\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2660 - acc: 0.9056 - val_loss: 0.3495 - val_acc: 0.8801\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.2643 - acc: 0.9061 - val_loss: 0.3463 - val_acc: 0.8799\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2603 - acc: 0.9077 - val_loss: 0.3470 - val_acc: 0.8789\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.2566 - acc: 0.9094 - val_loss: 0.3507 - val_acc: 0.8771\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.2529 - acc: 0.9096 - val_loss: 0.3457 - val_acc: 0.8785\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.2478 - acc: 0.9130 - val_loss: 0.3401 - val_acc: 0.8822\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.2468 - acc: 0.9124 - val_loss: 0.3508 - val_acc: 0.8771\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.2414 - acc: 0.9140 - val_loss: 0.3436 - val_acc: 0.8801\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2392 - acc: 0.9160 - val_loss: 0.3466 - val_acc: 0.8788\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.2368 - acc: 0.9166 - val_loss: 0.3519 - val_acc: 0.8795\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2331 - acc: 0.9168 - val_loss: 0.3514 - val_acc: 0.8803\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.2311 - acc: 0.9190 - val_loss: 0.3494 - val_acc: 0.8809\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.2270 - acc: 0.9201 - val_loss: 0.3463 - val_acc: 0.8823\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.2232 - acc: 0.9215 - val_loss: 0.3529 - val_acc: 0.8814\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.2230 - acc: 0.9205 - val_loss: 0.3559 - val_acc: 0.8814\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2196 - acc: 0.9231 - val_loss: 0.3532 - val_acc: 0.8793\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.2176 - acc: 0.9234 - val_loss: 0.3529 - val_acc: 0.8831\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.2147 - acc: 0.9244 - val_loss: 0.3446 - val_acc: 0.8817\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.2124 - acc: 0.9245 - val_loss: 0.3869 - val_acc: 0.8692\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.2083 - acc: 0.9263 - val_loss: 0.3482 - val_acc: 0.8832\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2084 - acc: 0.9259 - val_loss: 0.3475 - val_acc: 0.8840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f71fe84bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwYQHzWno1ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}